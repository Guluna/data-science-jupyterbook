{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e84bc42d",
   "metadata": {},
   "source": [
    "# Chapter 3- Advanced Preprocessing Techniques\n",
    "\n",
    "## Merging DataFrames\n",
    "\n",
    "A critical aspect in data analysis and feature engineering is combining multiple datasets because more often, it's spread across several sources, waiting to be collected, organized, and analyzed. Let's learn how to use this powerful tool to combine DataFrames and discover the various merge operations and their usage in different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07aea8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_books = pd.DataFrame({\n",
    "     \"Book_ID\": [1, 2, 3, 4, 5],\n",
    "     \"Book_Title\": ['Gatsby', 'Mockingbird', '1984', 'Catcher', 'LOTR'],\n",
    "     \"Author_ID\": [101, 102, 103, None, 112],\n",
    "     \"Genre\": ['Fiction', 'Fiction', 'Fiction', 'Fiction', 'Fantasy']\n",
    " })\n",
    "\n",
    "# creating the DataFrame for Authors\n",
    "df_authors = pd.DataFrame({\n",
    "    \"Author_ID\": [101, 102, 103, 104, 105],\n",
    "    \"Author_Name\": ['F. Fitzgerald', 'H. Lee', 'G. Orwell', 'J. Salinger', 'J. Tolkien'],\n",
    "    \"Nationality\": ['American', 'American', 'British', 'American', 'British']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721a9522",
   "metadata": {},
   "source": [
    "Two important things to note:\n",
    "\n",
    "- The author with Author_ID=112 is missing in the df_authors dataframe\n",
    "- The book named Catcher in the df_books dataframe misses info about its author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cec5a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Book_ID   Book_Title  Author_ID    Genre    Author_Name Nationality\n",
      "0        1       Gatsby      101.0  Fiction  F. Fitzgerald    American\n",
      "1        2  Mockingbird      102.0  Fiction         H. Lee    American\n",
      "2        3         1984      103.0  Fiction      G. Orwell     British\n"
     ]
    }
   ],
   "source": [
    "# inner join will have only rows with common Author_ID in both dataframes, so we don't include books where author information is missing or undefined.\n",
    "merged_df = df_books.merge(df_authors, on=\"Author_ID\", how=\"inner\")\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f520ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Book_ID   Book_Title  Author_ID    Genre    Author_Name Nationality\n",
      "0      1.0       Gatsby      101.0  Fiction  F. Fitzgerald    American\n",
      "1      2.0  Mockingbird      102.0  Fiction         H. Lee    American\n",
      "2      3.0         1984      103.0  Fiction      G. Orwell     British\n",
      "3      NaN          NaN      104.0      NaN    J. Salinger    American\n",
      "4      NaN          NaN      105.0      NaN     J. Tolkien     British\n",
      "5      5.0         LOTR      112.0  Fantasy            NaN         NaN\n",
      "6      4.0      Catcher        NaN  Fiction            NaN         NaN\n"
     ]
    }
   ],
   "source": [
    "# outer join includes all the rows from both DataFrames and fills NaN for missing values\n",
    "merged_df = df_books.merge(df_authors, on=\"Author_ID\", how=\"outer\")\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94c93e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Book_ID   Book_Title  Author_ID    Genre    Author_Name Nationality\n",
      "0        1       Gatsby      101.0  Fiction  F. Fitzgerald    American\n",
      "1        2  Mockingbird      102.0  Fiction         H. Lee    American\n",
      "2        3         1984      103.0  Fiction      G. Orwell     British\n",
      "3        4      Catcher        NaN  Fiction            NaN         NaN\n",
      "4        5         LOTR      112.0  Fantasy            NaN         NaN\n"
     ]
    }
   ],
   "source": [
    "# left join includes all rows from the first DataFrame and fills NaN for missing values in the second DataFrame\n",
    "merged_df = df_books.merge(df_authors, on=\"Author_ID\", how=\"left\")\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6beda9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Book_ID   Book_Title  Author_ID    Genre    Author_Name Nationality\n",
      "0      1.0       Gatsby      101.0  Fiction  F. Fitzgerald    American\n",
      "1      2.0  Mockingbird      102.0  Fiction         H. Lee    American\n",
      "2      3.0         1984      103.0  Fiction      G. Orwell     British\n",
      "3      NaN          NaN      104.0      NaN    J. Salinger    American\n",
      "4      NaN          NaN      105.0      NaN     J. Tolkien     British\n"
     ]
    }
   ],
   "source": [
    "# right join includes all rows from the second DataFrame, in reverse to a left join.\n",
    "merged_df = df_books.merge(df_authors, on=\"Author_ID\", how=\"right\")\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0719b20",
   "metadata": {},
   "source": [
    "## Concat DataFrames\n",
    "\n",
    "ignore_index=True parameter? When set to True, it resets the index in the resulting DataFrame. So, in the resultant DataFrame, the indices are in increasing order starting from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb0bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name  Age           City IsYouthful\n",
      "0   John   28       New York        Yes\n",
      "1   Anna   24    Los Angeles        Yes\n",
      "2  Peter   33         Berlin         No\n",
      "3  Megan   34  San Francisco         No\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df1\n",
    "data_dict = {\"Name\": [\"John\", \"Anna\", \"Peter\"],\n",
    "             \"Age\": [28, 24, 33],\n",
    "             \"City\": [\"New York\", \"Los Angeles\", \"Berlin\"]}\n",
    "\n",
    "df1 = pd.DataFrame(data_dict)\n",
    "isYouthful = lambda age: \"Yes\" if age < 30 else \"No\"\n",
    "df1[\"IsYouthful\"] = df1[\"Age\"].apply(isYouthful)\n",
    "\n",
    "# df2\n",
    "df2 = pd.DataFrame({\"Name\": [\"Megan\"], \"Age\": [34], \"City\": [\"San Francisco\"], \"IsYouthful\": [\"No\"]})\n",
    "\n",
    "# concat df1 and df2\n",
    "df_concatenated = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "print(df_concatenated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5298c705",
   "metadata": {},
   "source": [
    "# Grouping in Pandas\n",
    "\n",
    "Grouping rows of a DataFrame is a powerful tool that allows you to aggregate rows based on the values in one or more columns of your data. The groupby function in pandas is the basis of group operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b693b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name  Age      City\n",
      "0     Alex   12  New York\n",
      "4     Alex   21  New York\n",
      "5  Charlie   35  New York\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    \"Name\": [\"Alex\", \"Bob\", \"Chloe\", \"Charlie\", \"Alex\", \"Charlie\"],\n",
    "    \"Age\": [12, 15, 28, 55, 21, 35],\n",
    "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Los Angeles\", \"New York\", \"New York\"]\n",
    "})\n",
    "\n",
    "# the grouped variable is a special pandas DataFrameGroupBy object that has divided our DataFrame into groups by city. It's like a dictionary: \n",
    "# each key is a unique city from our City column, and the corresponding value for each key is a DataFrame comprising all rows with that city in the City column.\n",
    "grouped = df.groupby(\"City\")\n",
    "\n",
    "print(grouped.get_group(\"New York\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1fda36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Group Key: FB\n",
      "  Company Person  Sales\n",
      "4      FB   Carl    243\n",
      "5      FB  Sarah    350\n",
      "\n",
      "Group Key: GOOG\n",
      "  Company   Person  Sales\n",
      "0    GOOG      Sam    200\n",
      "1    GOOG  Charlie    120\n",
      "\n",
      "Group Key: MSFT\n",
      "  Company   Person  Sales\n",
      "2    MSFT      Amy    340\n",
      "3    MSFT  Vanessa    124\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Create a simple dataframe\n",
    "data = {'Company': ['GOOG', 'GOOG', 'MSFT', 'MSFT', 'FB', 'FB'],\n",
    "       'Person': ['Sam', 'Charlie', 'Amy', 'Vanessa', 'Carl', 'Sarah'],\n",
    "       'Sales': [200, 120, 340, 124, 243, 350]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply groupby\n",
    "df_grouped = df.groupby('Company')\n",
    "\n",
    "for key, item in df_grouped:\n",
    "    print(\"\\nGroup Key: {}\".format(key))\n",
    "    print(item)\n",
    "    # print(df_grouped.get_group(key), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c139bf",
   "metadata": {},
   "source": [
    "The  benefit of the groupby method is the variety of operations we can perform on the groupby object. Functions like sum(), mean(), etc., help us simplify the grouped data into more insightful information. Here's how we can use groupby and find out the total sales for each company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e95e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Person  Sales\n",
      "Company                   \n",
      "FB        CarlSarah    593\n",
      "GOOG     SamCharlie    320\n",
      "MSFT     AmyVanessa    464\n"
     ]
    }
   ],
   "source": [
    "grouped = df.groupby('Company')\n",
    "print(grouped.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ea7de",
   "metadata": {},
   "source": [
    "## Groupby(), apply() and lambda fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df228637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company\n",
      "FB      350\n",
      "GOOG    200\n",
      "MSFT    340\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ll/1x3pcpw96cn3pc3l3hqztlvr0000gn/T/ipykernel_55548/4292066780.py:2: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  print(df.groupby('Company').apply(lambda x: x['Sales'].max()))\n"
     ]
    }
   ],
   "source": [
    "# using apply() on groupby object\n",
    "print(df.groupby('Company').apply(lambda x: x['Sales'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b85232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Person  Sales\n",
      "Company                \n",
      "FB         Sarah    350\n",
      "GOOG         Sam    200\n",
      "MSFT     Vanessa    340\n"
     ]
    }
   ],
   "source": [
    "# same thing as above\n",
    "grouped = df.groupby('Company')\n",
    "print(grouped.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63921d4e",
   "metadata": {},
   "source": [
    "## Applying Aggregation Functions after Grouping or Iterating Through Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d95cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City\n",
      "Chicago        28.000000\n",
      "Los Angeles    35.000000\n",
      "New York       22.666667\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "City: Chicago\n",
      "Number of people: 1\n",
      "Average age: 28.0\n",
      "\n",
      "City: Los Angeles\n",
      "Number of people: 2\n",
      "Average age: 35.0\n",
      "\n",
      "City: New York\n",
      "Number of people: 3\n",
      "Average age: 22.666666666666668\n"
     ]
    }
   ],
   "source": [
    "print(grouped['Age'].mean())\n",
    "\n",
    "for name, group in grouped:\n",
    "    print(\"\\nCity:\", name)\n",
    "    print(\"Number of people:\", len(group))\n",
    "    print(\"Average age:\", group[\"Age\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1a113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3aeb4459",
   "metadata": {},
   "source": [
    "# Filtering Grouped DataFrame Using Boolean Lambda Function\n",
    "\n",
    "Boolean selection does not apply to grouped dataframes. Instead, we use the filter() function, which takes a boolean function as an argument. For instance, let's keep products with a summary quantity greater than 90."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2d7810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Product   Store  Quantity\n",
      "0   Apple  Store1        20\n",
      "3   Apple  Store2        50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sales = pd.DataFrame({\n",
    "  'Product': ['Apple', 'Banana', 'Pear', 'Apple', 'Banana', 'Pear'],\n",
    "  'Store': ['Store1', 'Store1', 'Store1', 'Store2', 'Store2', 'Store2'],\n",
    "  'Quantity': [20, 30, 40, 50, 60, 70]\n",
    "})\n",
    "\n",
    "grouped = sales.groupby('Product')\n",
    "\n",
    "print(grouped.get_group('Apple'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d7ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Product   Store  Quantity\n",
      "2    Pear  Store1        40\n",
      "5    Pear  Store2        70\n"
     ]
    }
   ],
   "source": [
    "grouped = sales.groupby('Product')\n",
    "\n",
    "# filtering using boolean lambda fn\n",
    "filtered_df = grouped.filter(lambda x: x['Quantity'].sum() > 90)\n",
    "\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af8a916",
   "metadata": {},
   "source": [
    "# Master Sorting\n",
    "\n",
    "Let us see how to use Panda's sort_values() function for single and multi-column sorting and how to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "286fac04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Player  Points  Assists\n",
      "2  M. Jordan    32.0      4.2\n",
      "0   L. James    27.0      5.7\n",
      "1  K. Durant    26.0      4.7\n",
      "4  K. Bryant    26.0      7.4\n",
      "3   S. Curry    24.0      6.6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Player': ['L. James', 'K. Durant', 'M. Jordan',  'S. Curry', 'K. Bryant'],\n",
    "    'Points': [27.0, 26.0, 32.0, 24.0, 26.0],\n",
    "    'Assists': [5.7, 4.7, 4.2, 6.6, 7.4]\n",
    "})\n",
    "\n",
    "sorted_df = df.sort_values(by='Points', ascending=False)\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25753996",
   "metadata": {},
   "source": [
    "## Sorting on multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c01c9a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Player  Points  Assists\n",
      "2  M. Jordan    32.0      4.2\n",
      "0   L. James    27.0      5.7\n",
      "4  K. Bryant    26.0      7.4\n",
      "1  K. Durant    26.0      4.7\n",
      "3   S. Curry    24.0      6.6\n"
     ]
    }
   ],
   "source": [
    "# sorting on multiple columns\n",
    "sorted_df = df.sort_values(by=['Points', 'Assists'], ascending=False)\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d32f56",
   "metadata": {},
   "source": [
    "## Sorting on multiple columns in different order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d547e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Player  Points  Assists\n",
      "2  M. Jordan    32.0      4.2\n",
      "0   L. James    27.0      5.7\n",
      "4  K. Bryant    26.0      7.4\n",
      "1  K. Durant    26.0      4.7\n",
      "3   S. Curry    24.0      6.6\n"
     ]
    }
   ],
   "source": [
    "sorted_df = df.sort_values(by=['Points', 'Player'], ascending=[False, True])\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60bd4f6",
   "metadata": {},
   "source": [
    "# Random Quote\n",
    "\n",
    "\"It is wise to be cheerful\"\n",
    "\n",
    "\"Careful\" or any positive word might also fit well in the quote above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupybook_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
